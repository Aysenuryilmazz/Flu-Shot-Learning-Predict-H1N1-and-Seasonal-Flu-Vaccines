{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "training_features_data = pd.read_csv(\"../input/flu-shot-learning-h1n1-seasonal-flu-vaccines/training_set_features.csv\",\n",
    "                    sep=',')\n",
    "\n",
    "\n",
    "training_set_labels = pd.read_csv(\"../input/flu-shot-learning-h1n1-seasonal-flu-vaccines/training_set_labels.csv\",\n",
    "                    sep=',')\n",
    "\n",
    "\n",
    "test_features_data = pd.read_csv(\"../input/flu-shot-learning-h1n1-seasonal-flu-vaccines/test_set_features.csv\",\n",
    "                    sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_features_data.shape)  \n",
    "print(training_set_labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **here is preprocessing for train dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate null values\n",
    "\n",
    "#for float types\n",
    "training_features_data=training_features_data.fillna(training_features_data.mean())\n",
    "\n",
    "#for string types\n",
    "training_features_data=training_features_data.fillna('out-of-category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check no missing values are left \n",
    "training_features_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding categorical features (str-->float)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "\n",
    "enc.fit(training_features_data)\n",
    "training_features_data_arr=enc.transform(training_features_data)\n",
    "\n",
    "col_names_list=training_features_data.columns\n",
    "encoded_categorical_df=pd.DataFrame(training_features_data_arr, columns=col_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization(make all values bet. 0-1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(encoded_categorical_df)\n",
    "normalized_arr=scaler.transform(encoded_categorical_df)\n",
    "\n",
    "normalized_df=pd.DataFrame(normalized_arr, columns=col_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if data types are correct or not \n",
    "normalized_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **here is preprocessing for test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check types of test dataset\n",
    "test_features_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate null values\n",
    "\n",
    "#for float types\n",
    "test_features_data=test_features_data.fillna(test_features_data.mean())\n",
    "\n",
    "#for string types\n",
    "test_features_data=test_features_data.fillna('out-of-category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check no missing values are left \n",
    "test_features_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding categorical features  (str-->float)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(test_features_data)\n",
    "test_features_data_arr=enc.transform(test_features_data)\n",
    "\n",
    "col_names_list=test_features_data.columns\n",
    "test_encoded_categorical_df=pd.DataFrame(test_features_data_arr, columns=col_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data types\n",
    "test_encoded_categorical_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization(bet. 0-1)\n",
    "\n",
    "#using minmax scaler(look up)\n",
    "test_normalized_arr=scaler.transform(test_encoded_categorical_df)\n",
    "test_normalized_df=pd.DataFrame(test_normalized_arr, columns=col_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **here is regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn methods \n",
    "from sklearn.metrics import roc_curve, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df to X and Y\n",
    "y = training_set_labels.loc[:, 'h1n1_vaccine'].values\n",
    "X = normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 80-20 for training set / test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "\n",
    "# cross-validation with 5 splits\n",
    "cv = StratifiedShuffleSplit(n_splits=5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor-1: Decision Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree regressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "\n",
    "# parameters \n",
    "parameters = {\n",
    "                \"criterion\": [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "                \"splitter\": [\"best\",\"random\"],\n",
    "                }\n",
    "\n",
    "# grid search for parameters\n",
    "grid = GridSearchCV(estimator=regressor, param_grid=parameters, cv=cv, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "# detailed dataframe of gridsearch\n",
    "detailed_grid_results = pd.DataFrame(grid.cv_results_)\n",
    "detailed_grid_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test scores and return result string and indexes of false samples\n",
    "def display_test_scores(test, pred):\n",
    "    str_out = \"\"\n",
    "    str_out += (\"TEST SCORES\\n\")\n",
    "    str_out += (\"\\n\")\n",
    "\n",
    "    #print AUC score\n",
    "    auc = roc_auc_score(test, pred)\n",
    "    str_out += (\"AUC: {:.4f}\\n\".format(auc))\n",
    "    str_out += (\"\\n\")\n",
    "    \n",
    "    false_indexes = np.where(test != pred)\n",
    "    return str_out, false_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction results\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor-2: Bayesian-Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Ridge for regression \n",
    "\n",
    "clf_ridge = linear_model.BayesianRidge()\n",
    "\n",
    "\n",
    "# parameters \n",
    "parameters = {\n",
    "                'alpha_init': [None, 1],\n",
    "                'lambda_init': [1, 1e-3],\n",
    "            }\n",
    "\n",
    "\n",
    "# grid search for parameters\n",
    "grid = GridSearchCV(estimator=clf_ridge, param_grid=parameters, cv=cv, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\\n\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "# prediction results\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor-3: SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "# parameters \n",
    "parameters = {\n",
    "                'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                'C': [0.01,0.1,1,10,100],\n",
    "                'max_iter': [100,1000],\n",
    "            }\n",
    "\n",
    "# grid search for parameters\n",
    "grid = GridSearchCV(estimator=regr, param_grid=parameters, cv=cv, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\\n\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "# prediction results\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor-4: SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = SGDRegressor( tol=1e-3)\n",
    "\n",
    "\n",
    "# parameters \n",
    "parameters = {\n",
    "                'alpha': [0.0001, 0.001, 0.01, 1],\n",
    "                'max_iter': [10,100,1000],\n",
    "                'learning_rate': ['invscaling', 'optimal', 'adaptive'],\n",
    "            }\n",
    "\n",
    "# grid search for parameters\n",
    "grid = GridSearchCV(estimator=reg, param_grid=parameters, cv=cv, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\\n\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "\n",
    "# prediction results\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor-5: RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# parameters \n",
    "parameters = {\n",
    "                'n_estimators': [20, 50, 100],\n",
    "            }\n",
    "\n",
    "# grid search for parameters\n",
    "grid = GridSearchCV(estimator=rfr, param_grid=parameters, cv=cv, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\\n\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "\n",
    "\n",
    "# prediction results\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  if p-value<=0.05 --> difference of two model is significant(yani iki modelin farkı belirgin, yani iki model farklı)\n",
    "\n",
    ">  if p-value>0.05 --> difference of two model is NOT significant(yani iki model çok farklı değil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy import stats\n",
    "\n",
    "#test_size=%20\n",
    "n_splits = 5\n",
    "\n",
    "#test_size_list=[0.50, .30, .10]\n",
    "\n",
    "#for i in test_size_list:\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, random_state=42, test_size=0.2)\n",
    "\n",
    "model_1 = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "model_2 = SGDRegressor(alpha= 0.001, learning_rate='adaptive', max_iter=100)\n",
    "model_3 = linear_model.BayesianRidge(alpha_init=None, lambda_init= 0.001)\n",
    "\n",
    "\n",
    "cv_mae_1 = []\n",
    "cv_mae_2 = []\n",
    "cv_mae_3 = []\n",
    "\n",
    "\n",
    "\n",
    "for X_train_list, X_test_list in sss.split(X,y):\n",
    "    model_1.fit(X.loc[X_train_list], y[X_train_list])\n",
    "    pred_1 = model_1.predict(X.loc[X_test_list])\n",
    "    err_1 = mean_absolute_error(y[X_test_list], pred_1)\n",
    "    cv_mae_1.append(err_1)\n",
    "\n",
    "\n",
    "    model_2.fit(X.loc[X_train_list], y[X_train_list])\n",
    "    pred_2 = model_2.predict(X.loc[X_test_list])\n",
    "    err_2 = mean_absolute_error(y[X_test_list], pred_2)\n",
    "    cv_mae_2.append(err_2)\n",
    "\n",
    "    model_3.fit(X.loc[X_train_list], y[X_train_list])\n",
    "    pred_3 = model_3.predict(X.loc[X_test_list])\n",
    "    err_3 = mean_absolute_error(y[X_test_list], pred_3)\n",
    "    cv_mae_3.append(err_3)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(stats.ttest_rel(cv_mae_1,cv_mae_2))\n",
    "print(stats.ttest_rel(cv_mae_3,cv_mae_2))\n",
    "print(stats.ttest_rel(cv_mae_3,cv_mae_1))\n",
    "\n",
    "#üç modeli karşılaştırdık; hepsi significant çıktı, en büyük olan modeli seçiyoruz, eyv:dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Ridge for regression \n",
    "\n",
    "#clf_ridge = linear_model.BayesianRidge(alpha_init=None, lambda_init=0.001)\n",
    "#clf_ridge.fit(X,y)\n",
    "\n",
    "# prediction results\n",
    "#y_pred = clf_ridge.predict(test_normalized_df)\n",
    "\n",
    "#y_pred = 1/(1+np.exp(-y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest regressor\n",
    "\n",
    "rfr = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "rfr.fit(X,y)\n",
    "\n",
    "# prediction results\n",
    "y_pred = rfr.predict(test_normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.sum(np.logical_or(np.array(y_pred) > 1, np.array(y_pred) < 0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
